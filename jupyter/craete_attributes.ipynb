{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from pathlib import Path\n",
    "base_dir = Path(\"/data1/ujiie/shinra/tohoku_bert\")\n",
    "for base_fn in base_dir.glob(\"**/**/tokens\"):\n",
    "    try:\n",
    "        category = base_fn.parent.stem\n",
    "        fn = base_fn.parent\n",
    "        with open(fn / \"attributes.txt\", \"r\") as f:\n",
    "            attributes = f.read()\n",
    "        with open(base_dir / \"attributes\" / f\"{category}.txt\", \"w\") as f:\n",
    "            f.write(attributes)\n",
    "    except:\n",
    "        print(base_fn.parent.stem, base_fn.parent.parent.stem)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Facility_Other Facility\n",
      "Museum Facility\n",
      "Car_Stop Facility\n",
      "Sports_Facility Facility\n",
      "Amusement_Park Facility\n",
      "Archaeological_Place_Other Facility\n",
      "Zoo Facility\n",
      "Tunnel Facility\n",
      "School Facility\n",
      "Worship_Place Facility\n",
      "Road_Facility Facility\n",
      "Shopping_Complex Facility\n",
      "Canal Facility\n",
      "Castle Facility\n",
      "Road Facility\n",
      "Park Facility\n",
      "Public_Institution Facility\n",
      "Railway_Facility Facility\n",
      "Medical_Institution Facility\n",
      "Port Facility\n",
      "Palace Facility\n",
      "Research_Institute Facility\n",
      "Station Facility\n",
      "Theater Facility\n",
      "Military_Base Facility\n",
      "Cemetery Facility\n",
      "Dam Facility\n",
      "Power_Plant Facility\n",
      "Railroad Facility\n",
      "FOE_Other Facility\n",
      "Bridge Facility\n",
      "Line_Other Facility\n",
      "Company JP-5\n",
      "Compound JP-5\n",
      "Person JP-5\n",
      "Sports_Team Organization\n",
      "Show_Organization Organization\n",
      "Political_Party Organization\n",
      "Organization_Other Organization\n",
      "Political_Organization_Other Organization\n",
      "Military Organization\n",
      "Sports_Federation Organization\n",
      "Sports_League Organization\n",
      "Nonprofit_Organization Organization\n",
      "International_Organization Organization\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import json\n",
    "annotations = {}\n",
    "base_dir = Path(\"/data1/ujiie/shinra/tohoku_bert/\")\n",
    "for parent_dir in base_dir.glob(\"**/**/tokens\"):\n",
    "    category = parent_dir.parent.stem\n",
    "    anns = {}\n",
    "    with open(parent_dir.parent / f\"{category}_dist.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line = json.loads(line)\n",
    "            if \"text_offset\" not in line:\n",
    "                continue\n",
    "            anns[(int(line[\"page_id\"]), line[\"token_offset\"][\"start\"][\"line_id\"], line[\"token_offset\"][\"start\"][\"offset\"])] = line[\"attribute\"]\n",
    "        annotations[category] = anns\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from collections import defaultdict\n",
    "cdir = Path(\"/home/is/ujiie/shinra-pipeline/outputs\")\n",
    "attr_path = Path(\"/data1/ujiie/shinra/tohoku_bert/attributes\")\n",
    "output_dir = Path(\"/home/is/ujiie/shinra-pipeline/attributes\")\n",
    "# attr_list = [\"Nonprofit_Organization\", \"Political_Party\", \"Show_Organization\", \"Sports_Federation\", \"Bridge\", \"Car_Stop\", \"Castle\", \"Company\"]\n",
    "attr_list = []\n",
    "for fn in cdir.glob(\"*.jsonl\"):\n",
    "    category = fn.stem\n",
    "    # if category != \"Person\":\n",
    "    #     continue\n",
    "\n",
    "    with open(attr_path / f\"{category}.txt\", \"r\") as f:\n",
    "        attributes = [line for line in f.read().split(\"\\n\") if line != \"\"]\n",
    "    \n",
    "    original_attr = set(attributes)\n",
    "    alignments = {}\n",
    "    with open(fn, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line = json.loads(line)\n",
    "            if (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])) in annotations[category]:\n",
    "                query = (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"]))\n",
    "                if line[\"attribute\"] in alignments:\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "                else:\n",
    "                    alignments[line[\"attribute\"]] = defaultdict(int)\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "            elif (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])-1) in annotations[category]:\n",
    "                query = (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])-1)\n",
    "                if line[\"attribute\"] in alignments:\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "                else:\n",
    "                    alignments[line[\"attribute\"]] = defaultdict(int)\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "            elif (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])+1) in annotations[category]:\n",
    "                query = (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])+1)\n",
    "                if line[\"attribute\"] in alignments:\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "                else:\n",
    "                    alignments[line[\"attribute\"]] = defaultdict(int)\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "            elif (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])+2) in annotations[category]:\n",
    "                query = (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])+2)\n",
    "                if line[\"attribute\"] in alignments:\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "                else:\n",
    "                    alignments[line[\"attribute\"]] = defaultdict(int)\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "            elif (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])-2) in annotations[category]:\n",
    "                query = (line['page_id'], int(line[\"token_offset\"][\"start\"][\"line_id\"]), int(line[\"token_offset\"][\"start\"][\"offset\"])-2)\n",
    "                if line[\"attribute\"] in alignments:\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "                else:\n",
    "                    alignments[line[\"attribute\"]] = defaultdict(int)\n",
    "                    alignments[line[\"attribute\"]][annotations[category][query]] += 1\n",
    "\n",
    "    new_attributes = []\n",
    "    None_cnt = 0\n",
    "    for attr in attributes:\n",
    "        max_cat = None\n",
    "        max_num = -1\n",
    "        if attr in alignments:\n",
    "            for key, value in alignments[attr].items():\n",
    "                if category in attr_list:\n",
    "                    print(category, attr, key, value)\n",
    "                # print(attr, key, value)\n",
    "                if max_num < value and key not in new_attributes:\n",
    "                    max_cat = key\n",
    "                    max_num = value\n",
    "            if max_cat is not None:\n",
    "                new_attributes.append(max_cat)\n",
    "                # print(new_attributes)\n",
    "            else:\n",
    "                new_attributes.append(f\"None{None_cnt}\")\n",
    "                None_cnt += 1\n",
    "        else:\n",
    "            new_attributes.append(f\"None{None_cnt}\")\n",
    "            None_cnt += 1\n",
    "            print(category)\n",
    "    new_attributes_set = set(new_attributes)\n",
    "    assert len(new_attributes) == len(new_attributes_set)\n",
    "    with open(output_dir / f\"{category}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(new_attributes))\n",
    "        remain = original_attr - new_attributes_set\n",
    "        f.write(\"\\n\" + \",\".join(list(remain)))\n",
    "\n",
    "        # f.write(\"\\n\\n\" + \"\\n\".join(attributes))\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Park\n",
      "Archaeological_Place_Other\n",
      "Bridge\n",
      "Bridge\n",
      "Power_Plant\n",
      "Power_Plant\n",
      "Power_Plant\n",
      "Canal\n",
      "Canal\n",
      "Canal\n",
      "Public_Institution\n",
      "Show_Organization\n",
      "Show_Organization\n",
      "Car_Stop\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Castle\n",
      "Sports_Federation\n",
      "Sports_Federation\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Cemetery\n",
      "Research_Institute\n",
      "Dam\n",
      "Dam\n",
      "Dam\n",
      "Sports_Team\n",
      "Sports_Team\n",
      "Sports_Team\n",
      "Road\n",
      "Road\n",
      "Road\n",
      "FOE_Other\n",
      "FOE_Other\n",
      "FOE_Other\n",
      "Road_Facility\n",
      "School\n",
      "School\n",
      "School\n",
      "School\n",
      "Facility_Other\n",
      "Line_Other\n",
      "Line_Other\n",
      "Line_Other\n",
      "Line_Other\n",
      "Line_Other\n",
      "Shopping_Complex\n",
      "Medical_Institution\n",
      "Station\n",
      "Worship_Place\n",
      "Worship_Place\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('anaconda3-2019.10': pyenv)"
  },
  "interpreter": {
   "hash": "e51c5f256d71b2b33a64ccdd4da515bbaa93decd18c7ffd011d49e5cf27acb29"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}